{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/AISaturdaysKigali/intro-to-ai/blob/master/lectures/w06_neural_network_pytorch/building_nn_model_pytorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a neural network model in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in this chapter, you have learned about the basic utility components of PyTorch for manipulating tensors and organizing data into formats that we can iterate over during training. In this section, we will finally implement our first predictive model in PyTorch. As PyTorch is a bit more flexible but also more complex than machine learning libraries such as scikit-learn, we will start with a simple linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The PyTorch neural network module (torch.nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn` is an elegantly designed module developed to help create and train NNs. It allows easy prototyping and the building of complex models in just a few lines of code.\n",
    "\n",
    "To fully utilize the power of the module and customize it for your problem, you need to understand what it’s doing. To develop this understanding, we will first train a basic linear regression model on a toy dataset without using any features from the `torch.nn` module; we will use nothing but the basic\n",
    "PyTorch tensor operations.\n",
    "\n",
    "Then, we will incrementally add features from `torch.nn` and `torch.optim`. As you will see in the following subsections, these modules make building an NN model extremely easy. We will also take advantage of the dataset pipeline functionalities supported in PyTorch, such as `Dataset` and `DataLoader`,\n",
    "which you learned about in the previous section. In this book, we will use the torch.nn module to build NN models.\n",
    "\n",
    "The most commonly used approach for building an NN in PyTorch is through nn.Module, which allows layers to be stacked to form a network. This gives us more control over the forward pass. We will see examples of building an NN model using the `nn.Module` class.\n",
    "\n",
    "Finally, as you will see in the following subsections, a trained model can be saved and reloaded for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection, we will build a simple model to solve a linear regression problem. First, let’s create a toy dataset in NumPy and visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the training examples will be shown in a scatterplot as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.arange(10, dtype='float32').reshape((10, 1))\n",
    "y_train = np.array([1.0, 1.3, 3.1, 2.0, 5.0, 6.3, 6.6, \n",
    "                    7.4, 8.0, 9.0], dtype='float32')\n",
    "\n",
    "plt.plot(X_train, y_train, 'o', markersize=10)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "#plt.savefig('figures/12_07.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will standardize the features (mean centering and dividing by the standard deviation) and create a PyTorch `Dataset` for the training set and a corresponding `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)\n",
    "X_train_norm = torch.from_numpy(X_train_norm)\n",
    "\n",
    "# On some computers the explicit cast to .float() is\n",
    "# necessary\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_ds = TensorDataset(X_train_norm, y_train)\n",
    "\n",
    "batch_size = 1\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we set a batch size of 1 for the `DataLoader`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define our model for linear regression as $z = wx + b$. Here, we are going to use the `torch.nn` module. It provides predefined layers for building complex NN models, but to start, you will learn how to define a model from scratch. Later in this chapter, you will see how to use those predefined layers.\n",
    "\n",
    "For this regression problem, we will define a linear regression model from scratch. We will define the parameters of our model, `weight` and `bias`, which correspond to the weight and the bias parameters, respectively. Finally, we will define the `model()` function to determine how this model uses the input\n",
    "data to generate its output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "weight = torch.randn(1)\n",
    "weight.requires_grad_()\n",
    "bias = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weight + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model, we can define the loss function that we want to minimize to find the optimal model weights. Here, we will choose the **mean squared error (MSE)** as our loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(input, target):\n",
    "    return (input-target).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, to learn the weight parameters of the model, we will use stochastic gradient descent. In this subsection, we will implement this training via the stochastic gradient descent procedure by ourselves, but in the next subsection, we will use the `SGD` method from the optimization package,\n",
    "`torch.optim`, to do the same thing.\n",
    "\n",
    "To implement the stochastic gradient descent algorithm, we need to compute the gradients. Rather than manually computing the gradients, we will use PyTorch’s `torch.autograd.backward` function.\n",
    "\n",
    "Now, we can set the learning rate and train the model for 200 epochs. The code for training the model against the batched version of the dataset is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 200\n",
    "log_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            weight -= weight.grad * learning_rate\n",
    "            bias -= bias.grad * learning_rate\n",
    "            weight.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    " \n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch}  Loss {loss.item():.4f}')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the trained model and plot it. For the test data, we will create a NumPy array of values evenly spaced between 0 and 9. Since we trained our model with standardized features, we will also apply the same standardization to the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final Parameters:', weight.item(), bias.item())\n",
    " \n",
    "X_test = np.linspace(0, 9, num=100, dtype='float32').reshape(-1, 1)\n",
    "X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)\n",
    "X_test_norm = torch.from_numpy(X_test_norm)\n",
    "y_pred = model(X_test_norm).detach().numpy()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(13, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "plt.plot(X_train_norm, y_train, 'o', markersize=10)\n",
    "plt.plot(X_test_norm, y_pred, '--', lw=3)\n",
    "plt.legend(['Training examples', 'Linear Reg.'], fontsize=15)\n",
    "ax.set_xlabel('x', size=15)\n",
    "ax.set_ylabel('y', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    " \n",
    "#plt.savefig('figures/12_08.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training via the torch.nn and torch.optim modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we saw how to train a model by writing a custom loss function `loss_fn()` and applied stochastic gradient descent optimization. However, writing the loss function and gradient updates can be a repeatable task across different projects. The `torch.nn` module provides a set of loss\n",
    "functions, and `torch.optim` supports most commonly used optimization algorithms that can be called to update the parameters based on the computed gradients. To see how they work, let’s create a new `MSE` loss function and a stochastic gradient descent optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we use the `torch.nn.Linear` class for the linear layer instead of manually defining it.\n",
    "\n",
    "Now, we can simply call the `step()` method of the `optimizer` to train the model. We can pass a batched dataset (such as `train_dl`, which was created in the previous example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        # 1. Generate predictions\n",
    "        pred = model(x_batch)[:, 0] \n",
    "\n",
    "        # 2. Calculate loss\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "\n",
    "        # 3. Compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Update parameters using gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # 5. Reset the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch}  Loss {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is trained, visualize the results and make sure that they are similar to the results of the previous method. To obtain the weight and bias parameters, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final Parameters:', model.weight.item(), model.bias.item())\n",
    " \n",
    "X_test = np.linspace(0, 9, num=100, dtype='float32').reshape(-1, 1)\n",
    "X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)\n",
    "X_test_norm = torch.from_numpy(X_test_norm)\n",
    "y_pred = model(X_test_norm)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(13, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "plt.plot(X_train_norm.detach().numpy(), y_train.detach().numpy(), 'o', markersize=10)\n",
    "plt.plot(X_test_norm.detach().numpy(), y_pred.detach().numpy(), '--', lw=3)\n",
    "plt.legend(['Training examples', 'Linear reg.'], fontsize=15)\n",
    "ax.set_xlabel('x', size=15)\n",
    "ax.set_ylabel('y', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    " \n",
    "#plt.savefig('ch12-linreg-2.pdf')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a multilayer perceptron for classifying flowers in the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, you saw how to build a model from scratch. We trained this model using stochastic gradient descent optimization. While we started our journey based on the simplest possible example, you can see that defining the model from scratch, even for such a simple case, is neither appealing nor good practice. PyTorch instead provides already defined layers through torch.nn that can be readily used as the building blocks of an NN model. In this section, you will learn how to use these layers to solve a classification task using the Iris flower dataset (identifying between three species of irises) and build a two-layer perceptron using the `torch.nn` module. First, let’s get the data from `sklearn.datasets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=1./3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we randomly select 100 samples (2/3) for training and 50 samples (1/3) for testing.\n",
    "\n",
    "Next, we standardize the features (mean centering and dividing by the standard deviation) and create a PyTorch `Dataset` for the training set and a corresponding `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)\n",
    "X_train_norm = torch.from_numpy(X_train_norm).float()\n",
    "y_train = torch.from_numpy(y_train) \n",
    "\n",
    "train_ds = TensorDataset(X_train_norm, y_train)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "batch_size = 2\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we set the batch size to 2 for the DataLoader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to use the `torch.nn` module to build a model efficiently. In particular, using the `nn.Module` class, we can stack a few layers and build an NN. You can see the list of all the layers that are already available at https://pytorch.org/docs/stable/nn.html. For this problem, we are going to use the Linear layer, which is also known as a fully connected layer or dense layer, and can be best represented by $f(w × x + b)$, where $x$ represents a tensor containing the input features, $w$ and $b$ are the weight matrix and the bias vector, and $f$ is the activation function.\n",
    "\n",
    "Each layer in an NN receives its inputs from the preceding layer; therefore, its dimensionality (rank and shape) is fixed. Typically, we need to concern ourselves with the dimensionality of output only when we design an NN architecture. Here, we want to define a model with two hidden layers. The first one receives an input of four features and projects them to 16 neurons. The second layer receives the output of the previous layer (which has a size of 16) and projects them to three output neurons, since we have three class labels. This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)  \n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        x = self.layer2(x)\n",
    "        x = nn.Softmax(dim=1)(x)\n",
    "        return x\n",
    "    \n",
    "input_size = X_train_norm.shape[1]\n",
    "hidden_size = 16\n",
    "output_size = 3\n",
    " \n",
    "model = Model(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we used the sigmoid activation function for the first layer and softmax activation for the last (output) layer. Softmax activation in the last layer is used to support multiclass classification since we have three class labels here (which is why we have three neurons in the output layer). We will discuss the different activation functions and their applications later in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify the loss function as **cross-entropy** loss and the optimizer as **Adam**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    " \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train the model. We will specify the number of epochs to be `100`. The code of training the flower classification model is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "loss_hist = [0] * num_epochs\n",
    "accuracy_hist = [0] * num_epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_fn(pred, y_batch.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        loss_hist[epoch] += loss.item()*y_batch.size(0)\n",
    "        is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "        accuracy_hist[epoch] += is_correct.sum()\n",
    "        \n",
    "    loss_hist[epoch] /= len(train_dl.dataset)\n",
    "    accuracy_hist[epoch] /= len(train_dl.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `loss_hist` and `accuracy_hist` lists keep the training loss and the training accuracy after each epoch. We can use this to visualize the learning curves as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(loss_hist, lw=3)\n",
    "ax.set_title('Training loss', size=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(accuracy_hist, lw=3)\n",
    "ax.set_title('Training accuracy', size=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('figures/12_09.pdf')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curves (training loss and training accuracy) are as follows above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the trained model on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the classification accuracy of the trained model on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)\n",
    "X_test_norm = torch.from_numpy(X_test_norm).float()\n",
    "y_test = torch.from_numpy(y_test) \n",
    "pred_test = model(X_test_norm)\n",
    "\n",
    "correct = (torch.argmax(pred_test, dim=1) == y_test).float()\n",
    "accuracy = correct.mean()\n",
    " \n",
    "print(f'Test Acc.: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we trained our model with standardized features, we also applied the same standardization to the test data. The classification accuracy is 0.98 (that is, 98 percent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and reloading the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained models can be saved on disk for future use. This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'iris_classifier.pt'\n",
    "torch.save(model, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `save(model)` will save both the model architecture and all the learned parameters. As a com-\n",
    "mon convention, we can save models using a `'pt'` or `'pth'` file extension.\n",
    "\n",
    "Now, let’s reload the saved model. Since we have saved both the model architecture and the weights, we can easily rebuild and reload the parameters in just one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = torch.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to verify the model architecture by calling `model_new.eval()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s evaluate this new model that is reloaded on the test dataset to verify that the results are the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model_new(X_test_norm)\n",
    "\n",
    "correct = (torch.argmax(pred_test, dim=1) == y_test).float()\n",
    "accuracy = correct.mean()\n",
    " \n",
    "print(f'Test Acc.: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to save only the learned parameters, you can use `save(model.state_dict())` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'iris_classifier_state.pt'\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reload the saved parameters, we first need to construct the model as we did before, then feed the loaded parameters to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = Model(input_size, hidden_size, output_size)\n",
    "model_new.load_state_dict(torch.load(path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
